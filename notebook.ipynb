{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJS5eE8ScyjN"
      },
      "source": [
        "# **1. Library Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34BDNqOddFvx",
        "outputId": "3a632abb-d325-415c-c70e-969e21e9c8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "竢ｬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "沒ｦ Installing...\n",
            "沒 Adjusting configuration...\n",
            "洸ｹ Patching environment...\n",
            "竢ｲ Done in 0:00:25\n",
            "沐 Restarting kernel...\n",
            "conda 22.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!conda --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h796lFG0dHUK",
        "outputId": "ade2dccf-9d75-4ac0-f266-4eb7a9470d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/stroke-detection\n",
            "\n",
            "  added / updated specs:\n",
            "    - python==3.9.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    libffi-3.3                 |       h58526e2_2          51 KB  conda-forge\n",
            "    openssl-1.1.1s             |       h0b41bf4_1         1.9 MB  conda-forge\n",
            "    python-3.9.1               |hffdb5ce_5_cpython        27.3 MB  conda-forge\n",
            "    setuptools-66.1.0          |     pyhd8ed1ab_0         628 KB  conda-forge\n",
            "    sqlite-3.40.0              |       h4ff8645_0         801 KB  conda-forge\n",
            "    tzdata-2022g               |       h191b570_0         106 KB  conda-forge\n",
            "    zlib-1.2.13                |       h166bdaf_4          92 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        31.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge None\n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu None\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.12.7-ha878542_0 None\n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.39-hcc3a1bd_1 None\n",
            "  libffi             conda-forge/linux-64::libffi-3.3-h58526e2_2 None\n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19 None\n",
            "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19 None\n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.40.0-h753d276_0 None\n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4 None\n",
            "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1 None\n",
            "  openssl            conda-forge/linux-64::openssl-1.1.1s-h0b41bf4_1 None\n",
            "  pip                conda-forge/noarch::pip-22.3.1-pyhd8ed1ab_0 None\n",
            "  python             conda-forge/linux-64::python-3.9.1-hffdb5ce_5_cpython None\n",
            "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0 None\n",
            "  setuptools         conda-forge/noarch::setuptools-66.1.0-pyhd8ed1ab_0 None\n",
            "  sqlite             conda-forge/linux-64::sqlite-3.40.0-h4ff8645_0 None\n",
            "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 None\n",
            "  tzdata             conda-forge/noarch::tzdata-2022g-h191b570_0 None\n",
            "  wheel              conda-forge/noarch::wheel-0.38.4-pyhd8ed1ab_0 None\n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 None\n",
            "  zlib               conda-forge/linux-64::zlib-1.2.13-h166bdaf_4 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "setuptools-66.1.0    | 628 KB    | : 100% 1.0/1 [00:00<00:00,  2.65it/s]\n",
            "python-3.9.1         | 27.3 MB   | : 100% 1.0/1 [00:07<00:00,  7.14s/it]               \n",
            "ca-certificates-2022 | 143 KB    | : 100% 1.0/1 [00:00<00:00, 20.81it/s]\n",
            "libffi-3.3           | 51 KB     | : 100% 1.0/1 [00:00<00:00, 17.92it/s]\n",
            "openssl-1.1.1s       | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  8.04it/s]\n",
            "zlib-1.2.13          | 92 KB     | : 100% 1.0/1 [00:00<00:00, 15.53it/s]\n",
            "sqlite-3.40.0        | 801 KB    | : 100% 1.0/1 [00:00<00:00,  5.31it/s]\n",
            "tzdata-2022g         | 106 KB    | : 100% 1.0/1 [00:00<00:00,  5.48it/s]\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate stroke-detection\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Retrieving notices: ...working... done\n"
          ]
        }
      ],
      "source": [
        "!conda create --name stroke-detection python==3.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziUiyuupdI4z",
        "outputId": "4884fcab-2c41-434a-fa51-f88bf31fca93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
            "To initialize your shell, run\n",
            "\n",
            "    $ conda init <SHELL_NAME>\n",
            "\n",
            "Currently supported shells are:\n",
            "  - bash\n",
            "  - fish\n",
            "  - tcsh\n",
            "  - xonsh\n",
            "  - zsh\n",
            "  - powershell\n",
            "\n",
            "See 'conda init --help' for more information and options.\n",
            "\n",
            "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda activate stroke-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bam0BC6IdKub",
        "outputId": "b021b83e-10bf-4973-e07d-0fd64b750b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tfx==1.11.0\n",
            "  Downloading tfx-1.11.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flask\n",
            "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-data-validation<1.12.0,>=1.11.0\n",
            "  Downloading tensorflow_data_validation-1.11.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tfx-bsl<1.12.0,>=1.11.0\n",
            "  Downloading tfx_bsl-1.11.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (21.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigquery<3,>=2.26.0\n",
            "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.43.0,>=0.42.0\n",
            "  Downloading tensorflow_model_analysis-0.42.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-transform<1.12.0,>=1.11.0\n",
            "  Downloading tensorflow_transform-1.11.0-py3-none-any.whl (446 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m446.5/446.5 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-pipelines-sdk==1.11.0\n",
            "  Downloading ml_pipelines_sdk-1.11.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py<2.0.0,>=0.9\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-tuner<2,>=1.0.4\n",
            "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click<8,>=7\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-apitools<1,>=0.5\n",
            "  Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.11.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting typing-extensions<5,>=3.10.0.2\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kubernetes<13,>=10.0.1\n",
            "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow\n",
            "  Downloading tensorflow-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-aiplatform<1.18,>=1.6.2\n",
            "  Downloading google_cloud_aiplatform-1.17.1-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portpicker<2,>=1.3.1\n",
            "  Downloading portpicker-1.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting apache-beam[gcp]<3,>=2.40\n",
            "  Downloading apache_beam-2.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<7,>=6\n",
            "  Downloading pyarrow-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m25.6/25.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2,>=1.28.1\n",
            "  Downloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<21,>=20\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2,>=1.16\n",
            "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2<4,>=2.7.3\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs<22,>=19.3.0\n",
            "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-metadata<1.12.0,>=1.11.0\n",
            "  Downloading ml_metadata-1.11.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-hub<0.13,>=0.9.0\n",
            "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client<2,>=1.8\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<5,>=4.1\n",
            "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<6,>=3.12\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m662.4/662.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core<1.33\n",
            "  Downloading google_api_core-1.32.0-py2.py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbconvert\n",
            "  Downloading nbconvert-7.2.8-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m274.8/274.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting notebook\n",
            "  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m439.1/439.1 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-console\n",
            "  Downloading jupyter_console-6.4.4-py3-none-any.whl (22 kB)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.0.4-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m137.8/137.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel\n",
            "  Downloading ipykernel-6.20.2-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six>=1.12.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libclang>=13.0.0\n",
            "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow) (65.5.1)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<4,>=3.13\n",
            "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Werkzeug>=2.2.2\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting flask\n",
            "  Downloading Flask-2.2.1-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Flask-2.2.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m101.1/101.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Flask-2.1.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Flask-2.1.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Flask-2.1.1-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading Flask-2.1.0-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m366.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Collecting flask\n",
            "  Downloading Flask-2.0.3-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0\n",
            "  Downloading orjson-3.8.5-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle~=2.2.0\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting crcmod<2.0,>=1.7\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydot<2,>=1.2.0\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m526.2/526.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tfx==1.11.0) (2.28.1)\n",
            "Collecting objsize<0.7.0,>=0.6.1\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy<2,>=1.16\n",
            "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2020.6.8\n",
            "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2018.3\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.8/site-packages (from apache-beam[gcp]<3,>=2.40->tfx==1.11.0) (0.19.0)\n",
            "Collecting httplib2<0.21.0,>=0.8\n",
            "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil<3,>=2.8.0\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-datastore<2,>=1.8.0\n",
            "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.14.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m241.5/241.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-bigquery-storage<2.14,>=2.6.3\n",
            "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.27.0-py2.py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m297.1/297.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.11.0-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
            "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-apitools<1,>=0.5\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1\n",
            "  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m268.7/268.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2\n",
            "  Downloading google_cloud_vision-3.3.0-py2.py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m386.5/386.5 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<5,>=3.1.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0\n",
            "  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m271.1/271.1 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
            "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n",
            "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-core<3,>=0.28.1\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-auth<3,>=1.18.0\n",
            "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0\n",
            "  Downloading googleapis_common_protos-1.58.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m223.0/223.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.18.0\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting oauth2client>=1.4.12\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
            "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m120.3/120.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m110.2/110.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Collecting ipython\n",
            "  Downloading ipython-8.8.0-py3-none-any.whl (775 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m775.8/775.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.8/site-packages (from kubernetes<13,>=10.0.1->tfx==1.11.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.8/site-packages (from kubernetes<13,>=10.0.1->tfx==1.11.0) (1.26.13)\n",
            "Collecting requests-oauthlib\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyparsing>=2.0.2\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorflow-metadata<1.12,>=1.11.0\n",
            "  Downloading tensorflow_metadata-1.11.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<2,>=1.0\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfarmhash<0.4,>=0.2\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-7.7.2-py2.py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m793.8/793.8 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-widgets<3,>=1.0.0\n",
            "  Downloading jupyterlab_widgets-1.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython-genutils~=0.2.0\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting widgetsnbextension~=3.6.0\n",
            "  Downloading widgetsnbextension-3.6.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets>=4.3.1\n",
            "  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m116.8/116.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting debugpy>=1.0\n",
            "  Downloading debugpy-1.6.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline>=0.1\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzmq>=17\n",
            "  Downloading pyzmq-25.0.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado>=6.1\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1\n",
            "  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
            "Collecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
            "  Downloading tensorflow_serving_api-2.10.1-py2.py3-none-any.whl (37 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n",
            "Collecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting defusedxml\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting bleach\n",
            "  Downloading bleach-5.0.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m160.9/160.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbformat>=5.1\n",
            "  Downloading nbformat-5.7.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m78.1/78.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core>=4.7\n",
            "  Downloading jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Collecting Send2Trash>=1.8.0\n",
            "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
            "Collecting nbclassic>=0.4.7\n",
            "  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting terminado>=0.8.3\n",
            "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
            "Collecting prometheus-client\n",
            "  Downloading prometheus_client-0.15.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
            "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.10.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.10.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.9.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-2.8.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.33.2-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.33.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_api_core-1.33.0-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0\n",
            "  Downloading google_cloud_dlp-3.10.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.10.0-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_dlp-3.9.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n",
            "  Downloading google_cloud_pubsub-2.13.12-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m238.3/238.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_pubsub-2.13.11-py2.py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-status>=1.16.0\n",
            "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
            "Collecting overrides<7.0.0,>=6.0.1\n",
            "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlparse>=0.3.0\n",
            "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n",
            "  Downloading google_cloud_spanner-3.26.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2\n",
            "  Downloading google_cloud_vision-3.2.0-py2.py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m386.5/386.5 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading google_cloud_vision-3.1.4-py2.py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m390.4/390.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting pexpect>4.3\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting entrypoints\n",
            "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
            "Collecting platformdirs>=2.5\n",
            "  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n",
            "Collecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
            "Collecting jupyter-server>=1.8\n",
            "  Downloading jupyter_server-2.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m365.2/365.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonschema>=2.6\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastjsonschema\n",
            "  Downloading fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
            "Collecting pyasn1>=0.1.7\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx==1.11.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tfx==1.11.0) (2.1.1)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ptyprocess\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
            "Collecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting grpcio-status>=1.16.0\n",
            "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
            "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
            "Collecting parso<0.9.0,>=0.8.0\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources>=1.4.0\n",
            "  Downloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
            "Collecting pkgutil-resolve-name>=1.3.10\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
            "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-server-terminals\n",
            "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
            "Collecting jupyter-events>=0.4.0\n",
            "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.1)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
            "Collecting rfc3986-validator>=0.1.1\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting python-json-logger>=2.0.4\n",
            "  Downloading python_json_logger-2.0.4-py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: google-apitools, crcmod, dill, pyfarmhash, docopt\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131022 sha256=ce18f48aa8d713358518a95e24b7b1a9fdbd5f8df7fd96a4692a630108596924\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/28/64/afa8e53e64b7c38a2e38ec9d346fed13944bd65ed6f7618b47\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=23421 sha256=8d26f5e3b1de411a09b95f03ac0eb268bc2562c2dbec18209a6ba31916590670\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/92/c3/6adef042a5224686e4ebdb9389aa568f1943c67abd8e05f7b9\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=85853d8c6fba09e156fa3183b385431917df2007249e97d98bd1c4bae0042784\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/f7/48/22bb9c8c35e6d86a02a8d3bc89f4462bc705ef75f6cbf84be0\n",
            "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp38-cp38-linux_x86_64.whl size=14288 sha256=0dfe12dd311ab98d9a6cb6bdc09d25f6655fe580c45a6ffc1d94e27e3aa6cde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/32/7f/1e591681730e2775f0a5dba2ff9741846ab3471e178e3f85ae\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=5697b567b2d47ab965855d66f0e35fcec3458daef2fbb7d9ae850ef9256ec4ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cc/e3/f1e272f628fdb013d969acc99cfe2e031ea15b3efb74ffe842\n",
            "Successfully built google-apitools crcmod dill pyfarmhash docopt\n",
            "Installing collected packages: webencodings, wcwidth, tensorboard-plugin-wit, Send2Trash, pytz, pyfarmhash, pyasn1, ptyprocess, pickleshare, mistune, libclang, kt-legacy, keras, ipython-genutils, flatbuffers, fastjsonschema, docopt, crcmod, backcall, zipp, wrapt, websocket-client, webcolors, uritemplate, uri-template, typing-extensions, traitlets, tornado, tinycss2, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sqlparse, soupsieve, sniffio, six, rsa, rfc3986-validator, regex, pyzmq, pyyaml, python-json-logger, pyrsistent, pyparsing, pymongo, pygments, pyasn1-modules, psutil, protobuf, prompt-toolkit, prometheus-client, platformdirs, pkgutil-resolve-name, pexpect, parso, pandocfilters, overrides, orjson, objsize, oauthlib, numpy, nest-asyncio, MarkupSafe, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, joblib, itsdangerous, grpcio, google-crc32c, gast, fqdn, fasteners, fastavro, entrypoints, dill, defusedxml, decorator, debugpy, cloudpickle, click, cachetools, attrs, absl-py, Werkzeug, terminado, tensorflow-hub, scipy, rfc3339-validator, requests-oauthlib, python-dateutil, pydot, pyarrow, proto-plus, portpicker, packaging, opt-einsum, ml-metadata, matplotlib-inline, keras-preprocessing, jupyter-core, jinja2, jedi, importlib-resources, importlib-metadata, httplib2, hdfs, h5py, googleapis-common-protos, google-resumable-media, google-pasta, google-auth, docker, comm, bleach, beautifulsoup4, astunparse, argon2-cffi-bindings, anyio, tensorflow-metadata, scikit-learn, qtpy, pandas, oauth2client, markdown, kubernetes, jupyter-server-terminals, jupyter-client, jsonschema, ipython, grpcio-status, google-auth-oauthlib, google-auth-httplib2, google-api-core, flask, arrow, argon2-cffi, apache-beam, tensorboard, nbformat, isoduration, ipykernel, grpc-google-iam-v1, google-cloud-core, google-apitools, google-api-python-client, tensorflow, qtconsole, nbclient, ml-pipelines-sdk, keras-tuner, jupyter-console, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-resource-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, tensorflow-serving-api, nbconvert, jupyter-events, google-cloud-pubsublite, google-cloud-aiplatform, jupyter-server, tfx-bsl, notebook-shim, tensorflow-transform, tensorflow-data-validation, nbclassic, notebook, widgetsnbextension, ipywidgets, tensorflow-model-analysis, jupyter, tfx\n",
            "Successfully installed MarkupSafe-2.1.2 Send2Trash-1.8.0 Werkzeug-2.2.2 absl-py-1.4.0 anyio-3.6.2 apache-beam-2.44.0 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 astunparse-1.6.3 attrs-21.4.0 backcall-0.2.0 beautifulsoup4-4.11.1 bleach-5.0.1 cachetools-4.2.4 click-7.1.2 cloudpickle-2.2.1 comm-0.1.2 crcmod-1.7 debugpy-1.6.5 decorator-5.1.1 defusedxml-0.7.1 dill-0.3.1.1 docker-4.4.4 docopt-0.6.2 entrypoints-0.4 fastavro-1.7.0 fasteners-0.18 fastjsonschema-2.16.2 flask-2.0.3 flatbuffers-23.1.4 fqdn-1.5.1 gast-0.4.0 google-api-core-1.32.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.4.6 google-cloud-aiplatform-1.17.1 google-cloud-bigquery-2.34.4 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.3 google-cloud-core-2.3.2 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.2 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.11 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-resource-manager-1.6.3 google-cloud-spanner-3.26.0 google-cloud-storage-2.7.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-3.1.4 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.4.1 googleapis-common-protos-1.58.0 grpc-google-iam-v1-0.12.6 grpcio-1.51.1 grpcio-status-1.48.2 h5py-3.7.0 hdfs-2.7.0 httplib2-0.20.4 importlib-metadata-6.0.0 importlib-resources-5.10.2 ipykernel-6.20.2 ipython-7.34.0 ipython-genutils-0.2.0 ipywidgets-7.7.2 isoduration-20.11.0 itsdangerous-2.1.2 jedi-0.18.2 jinja2-3.1.2 joblib-1.2.0 jsonpointer-2.3 jsonschema-4.17.3 jupyter-1.0.0 jupyter-client-7.4.9 jupyter-console-6.4.4 jupyter-core-5.1.3 jupyter-events-0.6.3 jupyter-server-2.1.0 jupyter-server-terminals-0.4.4 jupyterlab-pygments-0.2.2 jupyterlab-widgets-1.1.1 keras-2.10.0 keras-preprocessing-1.1.2 keras-tuner-1.1.3 kt-legacy-1.0.4 kubernetes-12.0.1 libclang-15.0.6.1 markdown-3.4.1 matplotlib-inline-0.1.6 mistune-2.0.4 ml-metadata-1.11.0 ml-pipelines-sdk-1.11.0 nbclassic-0.4.8 nbclient-0.7.2 nbconvert-7.2.8 nbformat-5.7.3 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 numpy-1.22.4 oauth2client-4.1.3 oauthlib-3.2.2 objsize-0.6.1 opt-einsum-3.3.0 orjson-3.8.5 overrides-6.5.0 packaging-20.9 pandas-1.5.3 pandocfilters-1.5.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 pkgutil-resolve-name-1.3.10 platformdirs-2.6.2 portpicker-1.5.2 prometheus-client-0.15.0 prompt-toolkit-3.0.36 proto-plus-1.22.2 protobuf-3.19.6 psutil-5.9.4 ptyprocess-0.7.0 pyarrow-6.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydot-1.4.2 pyfarmhash-0.3.2 pygments-2.14.0 pymongo-3.13.0 pyparsing-3.0.9 pyrsistent-0.19.3 python-dateutil-2.8.2 python-json-logger-2.0.4 pytz-2022.7.1 pyyaml-5.4.1 pyzmq-25.0.0 qtconsole-5.4.0 qtpy-2.3.0 regex-2022.10.31 requests-oauthlib-1.3.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rsa-4.9 scikit-learn-1.2.0 scipy-1.10.0 six-1.16.0 sniffio-1.3.0 soupsieve-2.3.2.post1 sqlparse-0.4.3 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.1 tensorflow-data-validation-1.11.0 tensorflow-estimator-2.10.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.29.0 tensorflow-metadata-1.11.0 tensorflow-model-analysis-0.42.0 tensorflow-serving-api-2.10.1 tensorflow-transform-1.11.0 termcolor-2.2.0 terminado-0.17.1 tfx-1.11.0 tfx-bsl-1.11.0 threadpoolctl-3.1.0 tinycss2-1.2.1 tornado-6.2 traitlets-5.8.1 typing-extensions-4.4.0 uri-template-1.2.0 uritemplate-3.0.1 wcwidth-0.2.6 webcolors-1.12 webencodings-0.5.1 websocket-client-1.4.2 widgetsnbextension-3.6.1 wrapt-1.14.1 zipp-3.11.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "google",
                  "ipython_genutils",
                  "pexpect",
                  "pickleshare"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install jupyter scikit-learn tensorflow tfx==1.11.0 flask joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqsJV1nsw7t",
        "outputId": "b9e96084-5212-4a40-8545-1a5ed09e0010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  Stroke-Disease-Detection.zip\n",
            "   creating: modules/\n",
            "  inflating: modules/components.py   \n",
            "  inflating: modules/trainer.py      \n",
            "  inflating: modules/transform.py    \n",
            "  inflating: modules/tuner.py        \n",
            "   creating: monitoring/\n",
            "  inflating: monitoring/Dockerfile   \n",
            "  inflating: monitoring/prometheus.yml  \n",
            " extracting: Dockerfile              \n",
            "   creating: config/\n",
            "  inflating: config/prometheus.config  \n",
            "   creating: images/\n"
          ]
        }
      ],
      "source": [
        "!unzip Stroke-Disease-Detection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j_mJt78ccznN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from typing import Text\n",
        "\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from modules import components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_695taudToL"
      },
      "source": [
        "# **2. Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsCH52-DhLIL"
      },
      "source": [
        "## 2.1 Environment and Kaggle Credential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4vE3L4J1cf"
      },
      "source": [
        "Set up the [Colab](https://colab.research.google.com) `operating system` environment with the `KAGGLE_USERNAME` variable and the `KAGGLE_KEY` variable to connect to the [Kaggle](https://kaggle.com) platform using [Kaggle's Beta API](https://www.kaggle.com/docs/api) Token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "S4RK1wiHe6UM"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'andrewbjamesie'\n",
        "os.environ['KAGGLE_KEY']      = 'b75c236a492526b84d0dc7517c37d48b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Msc4IG9hUbA"
      },
      "source": [
        "## 2.2 Dataset Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfGCMcvLhT8L"
      },
      "source": [
        "Download the dataset form Kaggle with the dataset file name, `healthcare-dataset-stroke-data.csv`. The dataset used in this project is the [Stroke Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) dataset in the form of a `.csv` ([Comma-separated Values](https://en.wikipedia.org/wiki/Comma-separated_values)) file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYO1ImDahIZN",
        "outputId": "c4b4c282-bdba-4413-bf0b-1ddbe3b7fcd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading healthcare-dataset-stroke-data.csv to /content\n",
            "\r  0% 0.00/310k [00:00<?, ?B/s]\n",
            "\r100% 310k/310k [00:00<00:00, 78.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d fedesoriano/stroke-prediction-dataset -f healthcare-dataset-stroke-data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNgt1eSiZs1g"
      },
      "source": [
        "## 2.3 Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xaa56p_2e_g6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
        "df = df.drop('id', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyb9l4Z2ZPb2",
        "outputId": "5186a5fd-2153-4652-ecdb-ec6c2fa2d6de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "gender                 0\n",
              "age                    0\n",
              "hypertension           0\n",
              "heart_disease          0\n",
              "ever_married           0\n",
              "work_type              0\n",
              "Residence_type         0\n",
              "avg_glucose_level      0\n",
              "bmi                  201\n",
              "smoking_status         0\n",
              "stroke                 0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUX4LgWjaAmV",
        "outputId": "56c0f9af-1c0c-4c07-96f9-af2584acd206"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "gender               0\n",
              "age                  0\n",
              "hypertension         0\n",
              "heart_disease        0\n",
              "ever_married         0\n",
              "work_type            0\n",
              "Residence_type       0\n",
              "avg_glucose_level    0\n",
              "bmi                  0\n",
              "smoking_status       0\n",
              "stroke               0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.dropna()\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYJ4qsHbaT_3",
        "outputId": "7e6fe3f7-896f-4f70-bb72-91a4970d5d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4909 entries, 0 to 5109\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   gender             4909 non-null   object \n",
            " 1   age                4909 non-null   float64\n",
            " 2   hypertension       4909 non-null   int64  \n",
            " 3   heart_disease      4909 non-null   int64  \n",
            " 4   ever_married       4909 non-null   object \n",
            " 5   work_type          4909 non-null   object \n",
            " 6   Residence_type     4909 non-null   object \n",
            " 7   avg_glucose_level  4909 non-null   float64\n",
            " 8   bmi                4909 non-null   float64\n",
            " 9   smoking_status     4909 non-null   object \n",
            " 10  stroke             4909 non-null   int64  \n",
            "dtypes: float64(3), int64(3), object(5)\n",
            "memory usage: 460.2+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qg_etYDeaSTh"
      },
      "outputs": [],
      "source": [
        "df['age'] = df['age'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "gO0iOIyLbRDv",
        "outputId": "13683978-8b9c-48a2-c2d6-1aa3cc8ee935"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2b320024-ee06-471c-a3a5-97144f9cd66c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Male</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Female</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>smokes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Female</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Male</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>186.21</td>\n",
              "      <td>29.0</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5104</th>\n",
              "      <td>Female</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>children</td>\n",
              "      <td>Rural</td>\n",
              "      <td>103.08</td>\n",
              "      <td>18.6</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5106</th>\n",
              "      <td>Female</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Urban</td>\n",
              "      <td>125.20</td>\n",
              "      <td>40.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5107</th>\n",
              "      <td>Female</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>82.99</td>\n",
              "      <td>30.6</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5108</th>\n",
              "      <td>Male</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>166.29</td>\n",
              "      <td>25.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5109</th>\n",
              "      <td>Female</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Govt_job</td>\n",
              "      <td>Urban</td>\n",
              "      <td>85.28</td>\n",
              "      <td>26.2</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4909 rows ﾃ 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b320024-ee06-471c-a3a5-97144f9cd66c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b320024-ee06-471c-a3a5-97144f9cd66c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b320024-ee06-471c-a3a5-97144f9cd66c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      gender  age  hypertension  heart_disease ever_married      work_type  \\\n",
              "0       Male   67             0              1          Yes        Private   \n",
              "2       Male   80             0              1          Yes        Private   \n",
              "3     Female   49             0              0          Yes        Private   \n",
              "4     Female   79             1              0          Yes  Self-employed   \n",
              "5       Male   81             0              0          Yes        Private   \n",
              "...      ...  ...           ...            ...          ...            ...   \n",
              "5104  Female   13             0              0           No       children   \n",
              "5106  Female   81             0              0          Yes  Self-employed   \n",
              "5107  Female   35             0              0          Yes  Self-employed   \n",
              "5108    Male   51             0              0          Yes        Private   \n",
              "5109  Female   44             0              0          Yes       Govt_job   \n",
              "\n",
              "     Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
              "0             Urban             228.69  36.6  formerly smoked       1  \n",
              "2             Rural             105.92  32.5     never smoked       1  \n",
              "3             Urban             171.23  34.4           smokes       1  \n",
              "4             Rural             174.12  24.0     never smoked       1  \n",
              "5             Urban             186.21  29.0  formerly smoked       1  \n",
              "...             ...                ...   ...              ...     ...  \n",
              "5104          Rural             103.08  18.6          Unknown       0  \n",
              "5106          Urban             125.20  40.0     never smoked       0  \n",
              "5107          Rural              82.99  30.6     never smoked       0  \n",
              "5108          Rural             166.29  25.6  formerly smoked       0  \n",
              "5109          Urban              85.28  26.2          Unknown       0  \n",
              "\n",
              "[4909 rows x 11 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATA_PATH = 'data'\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)\n",
        "\n",
        "df.to_csv(os.path.join(DATA_PATH, 'healthcare-dataset-stroke-data.csv'), index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTPMUlyUdR-M"
      },
      "source": [
        "# **3. Set Pipeline Variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "30gNVCymdTUL"
      },
      "outputs": [],
      "source": [
        "PIPELINE_NAME = 'stroke-disease-pipeline'\n",
        "\n",
        "# Pipeline inputs\n",
        "DATA_ROOT = 'data'\n",
        "TRANSFORM_MODULE_FILE = 'modules/transform.py'\n",
        "TUNER_MODULE_FILE = 'modules/tuner.py'\n",
        "TRAINER_MODULE_FILE = 'modules/trainer.py'\n",
        "\n",
        "# Pipeline outputs\n",
        "OUTPUT_BASE = 'outputs'\n",
        "\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sHcCOTDv5SV"
      },
      "source": [
        "# **4. Pipeline Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SI8vOQDEvjE5"
      },
      "outputs": [],
      "source": [
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "    \"\"\"Init local pipeline\n",
        "\n",
        "    Args:\n",
        "        components (dict): tfx components\n",
        "        pipeline_root (Text): path to pipeline directory\n",
        "\n",
        "    Returns:\n",
        "        pipeline.Pipeline: apache beam pipeline orchestration\n",
        "    \"\"\"\n",
        "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_args = [\n",
        "        '--direct_running_mode=multi_processing'\n",
        "        # 0 auto-detect based on on the number of CPUs available\n",
        "        # during execution time.\n",
        "        '----direct_num_workers=0'\n",
        "    ]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        eam_pipeline_args=beam_args\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K_QaH3t9wU2T",
        "outputId": "3bc3544a-e139-4f47-dc68-22ef0437ed54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 04m 23s]\n",
            "val_loss: 0.30945828557014465\n",
            "\n",
            "Best val_loss So Far: 0.15500961244106293\n",
            "Total elapsed time: 01h 00m 49s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n",
            "INFO:absl:Finished tuning... Tuner ID: tuner0\n",
            "INFO:absl:Best HyperParameters: {'space': [{'class_name': 'Choice', 'config': {'name': 'num_layers', 'default': 1, 'conditions': [], 'values': [1, 2, 3], 'ordered': True}}, {'class_name': 'Int', 'config': {'name': 'dense_units', 'default': None, 'conditions': [], 'min_value': 16, 'max_value': 256, 'step': 16, 'sampling': None}}, {'class_name': 'Float', 'config': {'name': 'dropout_rate', 'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.7, 'step': 0.1, 'sampling': None}}, {'class_name': 'Choice', 'config': {'name': 'learning_rate', 'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}}], 'values': {'num_layers': 1, 'dense_units': 48, 'dropout_rate': 0.6, 'learning_rate': 0.0001, 'tuner/epochs': 4, 'tuner/initial_epoch': 2, 'tuner/bracket': 2, 'tuner/round': 1, 'tuner/trial_id': '0003'}}\n",
            "INFO:absl:Best Hyperparameters are written to outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7/best_hyperparameters.txt.\n",
            "INFO:absl:Tuner results are written to outputs/stroke-disease-pipeline/Tuner/tuner_results/7/tuner_results.json.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 7 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'tuner_results': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Tuner/tuner_results/7\"\n",
            ", artifact_type: name: \"TunerResults\"\n",
            ")], 'best_hyperparameters': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7\"\n",
            ", artifact_type: name: \"HyperParameters\"\n",
            ")]}) for execution 7\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Tuner is finished.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in outputs/stroke-disease-pipeline/Tuner/.system/executor_execution/7/.temp/7/stroke_disaster_kt\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7ff256dcdc40>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "dense_units: 48\n",
            "dropout_rate: 0.6\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0003\n",
            "Score: 0.15500961244106293\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "dense_units: 48\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.15590107440948486\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "dense_units: 48\n",
            "dropout_rate: 0.6\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0013\n",
            "Score: 0.1566063016653061\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "dense_units: 48\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0012\n",
            "Score: 0.1580609381198883\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "dense_units: 48\n",
            "dropout_rate: 0.6\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 0.1581641137599945\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "dense_units: 240\n",
            "dropout_rate: 0.30000000000000004\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0018\n",
            "Score: 0.15890902280807495\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "dense_units: 48\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0000\n",
            "Score: 0.15927976369857788\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "dense_units: 80\n",
            "dropout_rate: 0.5\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0004\n",
            "Score: 0.16006919741630554\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "dense_units: 112\n",
            "dropout_rate: 0.5\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0002\n",
            "Score: 0.1611279994249344\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "dense_units: 240\n",
            "dropout_rate: 0.30000000000000004\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.16344670951366425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:node Trainer is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transformed_examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"hyperparameters\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Tuner\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Tuner\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"HyperParameters\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"best_hyperparameters\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"schema\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"SchemaGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.SchemaGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Schema\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"schema\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"transform_graph\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"TransformGraph\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transform_graph\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"trainer@outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"SchemaGen\"\n",
            "upstream_nodes: \"Transform\"\n",
            "upstream_nodes: \"Tuner\"\n",
            "downstream_nodes: \"Evaluator\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Trainer] Resolved inputs: ({'schema': [Artifact(artifact: id: 3\n",
            "type_id: 20\n",
            "uri: \"outputs/stroke-disease-pipeline/SchemaGen/schema/4\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290227927\n",
            "last_update_time_since_epoch: 1674290227927\n",
            ", artifact_type: id: 20\n",
            "name: \"Schema\"\n",
            ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
            "type_id: 28\n",
            "uri: \"outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293929776\n",
            "last_update_time_since_epoch: 1674293929776\n",
            ", artifact_type: id: 28\n",
            "name: \"HyperParameters\"\n",
            ")], 'transform_graph': [Artifact(artifact: id: 7\n",
            "type_id: 25\n",
            "uri: \"outputs/stroke-disease-pipeline/Transform/transform_graph/6\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290273756\n",
            "last_update_time_since_epoch: 1674290273756\n",
            ", artifact_type: id: 25\n",
            "name: \"TransformGraph\"\n",
            ")], 'examples': [Artifact(artifact: id: 12\n",
            "type_id: 15\n",
            "uri: \"outputs/stroke-disease-pipeline/Transform/transformed_examples/6\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"eval\\\", \\\"train\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290273757\n",
            "last_update_time_since_epoch: 1674290273757\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 8\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'schema': [Artifact(artifact: id: 3\n",
            "type_id: 20\n",
            "uri: \"outputs/stroke-disease-pipeline/SchemaGen/schema/4\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290227927\n",
            "last_update_time_since_epoch: 1674290227927\n",
            ", artifact_type: id: 20\n",
            "name: \"Schema\"\n",
            ")], 'hyperparameters': [Artifact(artifact: id: 14\n",
            "type_id: 28\n",
            "uri: \"outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293929776\n",
            "last_update_time_since_epoch: 1674293929776\n",
            ", artifact_type: id: 28\n",
            "name: \"HyperParameters\"\n",
            ")], 'transform_graph': [Artifact(artifact: id: 7\n",
            "type_id: 25\n",
            "uri: \"outputs/stroke-disease-pipeline/Transform/transform_graph/6\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290273756\n",
            "last_update_time_since_epoch: 1674290273756\n",
            ", artifact_type: id: 25\n",
            "name: \"TransformGraph\"\n",
            ")], 'examples': [Artifact(artifact: id: 12\n",
            "type_id: 15\n",
            "uri: \"outputs/stroke-disease-pipeline/Transform/transformed_examples/6\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"eval\\\", \\\"train\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290273757\n",
            "last_update_time_since_epoch: 1674290273757\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_run': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Trainer/model_run/8\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")]}), exec_properties={'train_args': '{\\n  \"num_steps\": 5000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}', 'module_path': 'trainer@outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl', 'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}'}, execution_output_uri='outputs/stroke-disease-pipeline/Trainer/.system/executor_execution/8/executor_output.pb', stateful_working_dir='outputs/stroke-disease-pipeline/Trainer/.system/stateful_working_dir/20230121-083653.609845', tmp_dir='outputs/stroke-disease-pipeline/Trainer/.system/executor_execution/8/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transformed_examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"hyperparameters\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Tuner\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Tuner\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"HyperParameters\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"best_hyperparameters\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"schema\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"SchemaGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.SchemaGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Schema\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"schema\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"transform_graph\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Transform\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Transform\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"TransformGraph\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"transform_graph\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"trainer@outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"SchemaGen\"\n",
            "upstream_nodes: \"Transform\"\n",
            "upstream_nodes: \"Tuner\"\n",
            "downstream_nodes: \"Evaluator\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"stroke-disease-pipeline\"\n",
            ", pipeline_run_id='20230121-083653.609845')\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "INFO:absl:udf_utils.get_fn {'train_args': '{\\n  \"num_steps\": 5000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}', 'module_path': 'trainer@outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl', 'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}'} 'run_fn'\n",
            "INFO:absl:Installing 'outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl' to a temporary directory.\n",
            "INFO:absl:Executing: ['/usr/bin/python3.real', '-m', 'pip', 'install', '--target', '/tmp/tmpqquhqy2h', 'outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl']\n",
            "INFO:absl:Successfully installed 'outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl'.\n",
            "INFO:absl:Training model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " gender_xf (InputLayer)         [(None, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " ever_married_xf (InputLayer)   [(None, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " work_type_xf (InputLayer)      [(None, 6)]          0           []                               \n",
            "                                                                                                  \n",
            " Residence_type_xf (InputLayer)  [(None, 3)]         0           []                               \n",
            "                                                                                                  \n",
            " smoking_status_xf (InputLayer)  [(None, 5)]         0           []                               \n",
            "                                                                                                  \n",
            " age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " hypertension_xf (InputLayer)   [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " heart_disease_xf (InputLayer)  [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " avg_glucose_level_xf (InputLay  [(None, 1)]         0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " bmi_xf (InputLayer)            [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 25)           0           ['gender_xf[0][0]',              \n",
            "                                                                  'ever_married_xf[0][0]',        \n",
            "                                                                  'work_type_xf[0][0]',           \n",
            "                                                                  'Residence_type_xf[0][0]',      \n",
            "                                                                  'smoking_status_xf[0][0]',      \n",
            "                                                                  'age_xf[0][0]',                 \n",
            "                                                                  'hypertension_xf[0][0]',        \n",
            "                                                                  'heart_disease_xf[0][0]',       \n",
            "                                                                  'avg_glucose_level_xf[0][0]',   \n",
            "                                                                  'bmi_xf[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 48)           1248        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 48)           2352        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 48)           0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1)            49          ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,649\n",
            "Trainable params: 3,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/2\n",
            "4996/5000 [============================>.] - ETA: 0s - loss: 0.2097 - binary_accuracy: 0.9367\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.95195, saving model to outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) Residence_type_xf with unsupported characters which will be renamed to residence_type_xf in the SavedModel.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5000/5000 [==============================] - 21s 4ms/step - loss: 0.2097 - binary_accuracy: 0.9367 - val_loss: 0.1636 - val_binary_accuracy: 0.9520\n",
            "Epoch 2/2\n",
            "4990/5000 [============================>.] - ETA: 0s - loss: 0.1509 - binary_accuracy: 0.9588\n",
            "Epoch 2: val_binary_accuracy did not improve from 0.95195\n",
            "5000/5000 [==============================] - 19s 4ms/step - loss: 0.1510 - binary_accuracy: 0.9588 - val_loss: 0.1580 - val_binary_accuracy: 0.9519\n",
            "INFO:tensorflow:struct2tensor is not available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:struct2tensor is not available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tensorflow_text is not available.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tensorflow_text is not available.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets\n",
            "INFO:absl:Training complete. Model written to outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving. ModelRun written to outputs/stroke-disease-pipeline/Trainer/model_run/8\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 8 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_run': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Trainer/model_run/8\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")]}) for execution 8\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Trainer is finished.\n",
            "INFO:absl:node Evaluator is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
            "    base_type: EVALUATE\n",
            "  }\n",
            "  id: \"Evaluator\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Evaluator\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"baseline_model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Latest_blessed_model_resolver\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Latest_blessed_model_resolver\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"blessing\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelBlessing\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"evaluation\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelEvaluation\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"eval_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"TruePositives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"FalsePositives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"TrueNegatives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"FalseNegatives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"stroke\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"gender\\\",\\n        \\\"ever_married\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"example_splits\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"fairness_indicator_thresholds\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Evaluator] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
            "type_id: 15\n",
            "uri: \"outputs/stroke-disease-pipeline/CsvExampleGen/examples/1\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:276776,xor_checksum:1674290211,sum_checksum:1674290211\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290219063\n",
            "last_update_time_since_epoch: 1674290219063\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'baseline_model': [], 'model': [Artifact(artifact: id: 15\n",
            "type_id: 30\n",
            "uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293981324\n",
            "last_update_time_since_epoch: 1674293981324\n",
            ", artifact_type: id: 30\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 9\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'examples': [Artifact(artifact: id: 1\n",
            "type_id: 15\n",
            "uri: \"outputs/stroke-disease-pipeline/CsvExampleGen/examples/1\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:276776,xor_checksum:1674290211,sum_checksum:1674290211\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674290219063\n",
            "last_update_time_since_epoch: 1674290219063\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")], 'baseline_model': [], 'model': [Artifact(artifact: id: 15\n",
            "type_id: 30\n",
            "uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293981324\n",
            "last_update_time_since_epoch: 1674293981324\n",
            ", artifact_type: id: 30\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Evaluator/blessing/9\"\n",
            ", artifact_type: name: \"ModelBlessing\"\n",
            ")], 'evaluation': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Evaluator/evaluation/9\"\n",
            ", artifact_type: name: \"ModelEvaluation\"\n",
            ")]}), exec_properties={'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"TruePositives\"\\n        },\\n        {\\n          \"class_name\": \"FalsePositives\"\\n        },\\n        {\\n          \"class_name\": \"TrueNegatives\"\\n        },\\n        {\\n          \"class_name\": \"FalseNegatives\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"stroke\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"gender\",\\n        \"ever_married\"\\n      ]\\n    }\\n  ]\\n}'}, execution_output_uri='outputs/stroke-disease-pipeline/Evaluator/.system/executor_execution/9/executor_output.pb', stateful_working_dir='outputs/stroke-disease-pipeline/Evaluator/.system/stateful_working_dir/20230121-083653.609845', tmp_dir='outputs/stroke-disease-pipeline/Evaluator/.system/executor_execution/9/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
            "    base_type: EVALUATE\n",
            "  }\n",
            "  id: \"Evaluator\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Evaluator\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"baseline_model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Latest_blessed_model_resolver\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Latest_blessed_model_resolver\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"blessing\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelBlessing\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"evaluation\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelEvaluation\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"eval_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"TruePositives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"FalsePositives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"TrueNegatives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"FalseNegatives\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"stroke\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"gender\\\",\\n        \\\"ever_married\\\"\\n      ]\\n    }\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"example_splits\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"fairness_indicator_thresholds\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"stroke-disease-pipeline\"\n",
            ", pipeline_run_id='20230121-083653.609845')\n",
            "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"TruePositives\"\\n        },\\n        {\\n          \"class_name\": \"FalsePositives\"\\n        },\\n        {\\n          \"class_name\": \"TrueNegatives\"\\n        },\\n        {\\n          \"class_name\": \"FalseNegatives\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"stroke\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"gender\",\\n        \"ever_married\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_eval_shared_model'\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"stroke\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "slicing_specs {\n",
            "  feature_keys: \"gender\"\n",
            "  feature_keys: \"ever_married\"\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TruePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalsePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TrueNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalseNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.5\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:Using outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving as  model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff25c8b8a00> and <keras.engine.input_layer.InputLayer object at 0x7ff25bc6fb50>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff25c8b8a00> and <keras.engine.input_layer.InputLayer object at 0x7ff25bc6fb50>).\n",
            "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
            "INFO:absl:Evaluating model.\n",
            "INFO:absl:udf_utils.get_fn {'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"TruePositives\"\\n        },\\n        {\\n          \"class_name\": \"FalsePositives\"\\n        },\\n        {\\n          \"class_name\": \"TrueNegatives\"\\n        },\\n        {\\n          \"class_name\": \"FalseNegatives\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"stroke\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"gender\",\\n        \"ever_married\"\\n      ]\\n    }\\n  ]\\n}'} 'custom_extractors'\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"stroke\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "slicing_specs {\n",
            "  feature_keys: \"gender\"\n",
            "  feature_keys: \"ever_married\"\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TruePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalsePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TrueNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalseNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.5\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"stroke\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "slicing_specs {\n",
            "  feature_keys: \"gender\"\n",
            "  feature_keys: \"ever_married\"\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TruePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalsePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TrueNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalseNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.5\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n",
            "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
            "model_specs {\n",
            "  label_key: \"stroke\"\n",
            "}\n",
            "slicing_specs {\n",
            "}\n",
            "slicing_specs {\n",
            "  feature_keys: \"gender\"\n",
            "  feature_keys: \"ever_married\"\n",
            "}\n",
            "metrics_specs {\n",
            "  metrics {\n",
            "    class_name: \"AUC\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Precision\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"Recall\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"ExampleCount\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TruePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalsePositives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"TrueNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"FalseNegatives\"\n",
            "  }\n",
            "  metrics {\n",
            "    class_name: \"BinaryAccuracy\"\n",
            "    threshold {\n",
            "      value_threshold {\n",
            "        lower_bound {\n",
            "          value: 0.5\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  model_names: \"\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff25b63a520> and <keras.engine.input_layer.InputLayer object at 0x7ff2569c8df0>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff25b63a520> and <keras.engine.input_layer.InputLayer object at 0x7ff2569c8df0>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff253ee9640> and <keras.engine.input_layer.InputLayer object at 0x7ff25c757e50>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff253ee9640> and <keras.engine.input_layer.InputLayer object at 0x7ff25c757e50>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff256d9dc10> and <keras.engine.input_layer.InputLayer object at 0x7ff2543829d0>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff256d9dc10> and <keras.engine.input_layer.InputLayer object at 0x7ff2543829d0>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff256c3c850> and <keras.engine.input_layer.InputLayer object at 0x7ff25be942e0>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff256c3c850> and <keras.engine.input_layer.InputLayer object at 0x7ff25be942e0>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff2537bcfd0> and <keras.engine.input_layer.InputLayer object at 0x7ff2537d61f0>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff2537bcfd0> and <keras.engine.input_layer.InputLayer object at 0x7ff2537d61f0>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff2531dfdc0> and <keras.engine.input_layer.InputLayer object at 0x7ff2531c8250>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff2531dfdc0> and <keras.engine.input_layer.InputLayer object at 0x7ff2531c8250>).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff251a92ee0> and <keras.engine.input_layer.InputLayer object at 0x7ff2531b7d90>).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x7ff251a92ee0> and <keras.engine.input_layer.InputLayer object at 0x7ff2531b7d90>).\n",
            "INFO:absl:Evaluation complete. Results written to outputs/stroke-disease-pipeline/Evaluator/evaluation/9.\n",
            "INFO:absl:Checking validation results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:absl:Blessing result True written to outputs/stroke-disease-pipeline/Evaluator/blessing/9.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 9 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'blessing': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Evaluator/blessing/9\"\n",
            ", artifact_type: name: \"ModelBlessing\"\n",
            ")], 'evaluation': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Evaluator/evaluation/9\"\n",
            ", artifact_type: name: \"ModelEvaluation\"\n",
            ")]}) for execution 9\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Evaluator is finished.\n",
            "INFO:absl:node Pusher is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model_blessing\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Evaluator\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Evaluator\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"ModelBlessing\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"blessing\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"outputs/serving_model\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Evaluator\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 15\n",
            "type_id: 30\n",
            "uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293981324\n",
            "last_update_time_since_epoch: 1674293981324\n",
            ", artifact_type: id: 30\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_blessing': [Artifact(artifact: id: 17\n",
            "type_id: 33\n",
            "uri: \"outputs/stroke-disease-pipeline/Evaluator/blessing/9\"\n",
            "custom_properties {\n",
            "  key: \"blessed\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model\"\n",
            "  value {\n",
            "    string_value: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model_id\"\n",
            "  value {\n",
            "    int_value: 15\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674294000633\n",
            "last_update_time_since_epoch: 1674294000633\n",
            ", artifact_type: id: 33\n",
            "name: \"ModelBlessing\"\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 10\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=10, input_dict={'model': [Artifact(artifact: id: 15\n",
            "type_id: 30\n",
            "uri: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674293981324\n",
            "last_update_time_since_epoch: 1674293981324\n",
            ", artifact_type: id: 30\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")], 'model_blessing': [Artifact(artifact: id: 17\n",
            "type_id: 33\n",
            "uri: \"outputs/stroke-disease-pipeline/Evaluator/blessing/9\"\n",
            "custom_properties {\n",
            "  key: \"blessed\"\n",
            "  value {\n",
            "    int_value: 1\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model\"\n",
            "  value {\n",
            "    string_value: \"outputs/stroke-disease-pipeline/Trainer/model/8\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"current_model_id\"\n",
            "  value {\n",
            "    int_value: 15\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"state\"\n",
            "  value {\n",
            "    string_value: \"published\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.11.0\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "create_time_since_epoch: 1674294000633\n",
            "last_update_time_since_epoch: 1674294000633\n",
            ", artifact_type: id: 33\n",
            "name: \"ModelBlessing\"\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Pusher/pushed_model/10\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"outputs/serving_model\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='outputs/stroke-disease-pipeline/Pusher/.system/executor_execution/10/executor_output.pb', stateful_working_dir='outputs/stroke-disease-pipeline/Pusher/.system/stateful_working_dir/20230121-083653.609845', tmp_dir='outputs/stroke-disease-pipeline/Pusher/.system/executor_execution/10/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"20230121-083653.609845\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"stroke-disease-pipeline.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  inputs {\n",
            "    key: \"model_blessing\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Evaluator\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"20230121-083653.609845\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"stroke-disease-pipeline.Evaluator\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"ModelBlessing\"\n",
            "          }\n",
            "        }\n",
            "        output_key: \"blessing\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"outputs/serving_model\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Evaluator\"\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "    enable_cache: true\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"stroke-disease-pipeline\"\n",
            ", pipeline_run_id='20230121-083653.609845')\n",
            "INFO:absl:Model version: 1674294000\n",
            "INFO:absl:Model written to serving path outputs/serving_model/1674294000.\n",
            "INFO:absl:Model pushed to outputs/stroke-disease-pipeline/Pusher/pushed_model/10.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 10 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"outputs/stroke-disease-pipeline/Pusher/pushed_model/10\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}) for execution 10\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:node Pusher is finished.\n"
          ]
        }
      ],
      "source": [
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "components = components.init_components({\n",
        "    'data_dir': DATA_ROOT,\n",
        "    'transform_module': TRANSFORM_MODULE_FILE,\n",
        "    'tuner_module': TUNER_MODULE_FILE,\n",
        "    'training_module': TRAINER_MODULE_FILE,\n",
        "    'training_steps': 5000,\n",
        "    'eval_steps': 1000,\n",
        "    'serving_model_dir': serving_model_dir\n",
        "})\n",
        "\n",
        "pipeline = init_local_pipeline(components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline=pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KlKcnFRwbNU",
        "outputId": "2119eeb9-ab5d-47ac-da1f-0eab8b21e211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: data/ (stored 0%)\n",
            "  adding: data/healthcare-dataset-stroke-data.csv (deflated 83%)\n",
            "  adding: images/ (stored 0%)\n",
            "  adding: images/model_plot.png (deflated 20%)\n",
            "  adding: outputs/ (stored 0%)\n",
            "  adding: outputs/serving_model/ (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/ (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/saved_model.pb (deflated 89%)\n",
            "  adding: outputs/serving_model/1674294000/assets/ (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/assets/vocab_compute_and_apply_vocabulary_1_vocabulary (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/assets/vocab_compute_and_apply_vocabulary_4_vocabulary (deflated 18%)\n",
            "  adding: outputs/serving_model/1674294000/assets/vocab_compute_and_apply_vocabulary_vocabulary (deflated 6%)\n",
            "  adding: outputs/serving_model/1674294000/assets/vocab_compute_and_apply_vocabulary_3_vocabulary (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/assets/vocab_compute_and_apply_vocabulary_2_vocabulary (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/keras_metadata.pb (deflated 91%)\n",
            "  adding: outputs/serving_model/1674294000/variables/ (stored 0%)\n",
            "  adding: outputs/serving_model/1674294000/variables/variables.index (deflated 63%)\n",
            "  adding: outputs/serving_model/1674294000/variables/variables.data-00000-of-00001 (deflated 25%)\n",
            "  adding: outputs/stroke-disease-pipeline/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/metadata.sqlite (deflated 93%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/tuner_results/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/tuner_results/7/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/tuner_results/7/tuner_results.json (deflated 88%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/best_hyperparameters/7/best_hyperparameters.txt (deflated 64%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Tuner/.system/executor_execution/7/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/_wheels/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Tuner-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl (deflated 13%)\n",
            "  adding: outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Trainer-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl (deflated 14%)\n",
            "  adding: outputs/stroke-disease-pipeline/_wheels/tfx_user_code_Transform-0.0+420d0b25a87e97753c5601655e878671b6d8a2d3908e307f81d59c59e6f5f087-py3-none-any.whl (deflated 14%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/saved_model.pb (deflated 89%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/vocab_compute_and_apply_vocabulary_1_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/vocab_compute_and_apply_vocabulary_4_vocabulary (deflated 18%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/vocab_compute_and_apply_vocabulary_vocabulary (deflated 6%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/vocab_compute_and_apply_vocabulary_3_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/assets/vocab_compute_and_apply_vocabulary_2_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/keras_metadata.pb (deflated 91%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/variables/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/variables/variables.index (deflated 63%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/pushed_model/10/variables/variables.data-00000-of-00001 (deflated 25%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Pusher/.system/executor_execution/10/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/1/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/1/Split-train/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/1/Split-train/data_tfrecord-00000-of-00001.gz (deflated 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/1/Split-eval/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/examples/1/Split-eval/data_tfrecord-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/.system/driver_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/.system/driver_execution/1674290214754369/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/CsvExampleGen/.system/executor_execution/1/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/2-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/5-00000-of-00001.gz (deflated 5%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/7-00000-of-00001.gz (deflated 2%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/METADATA-00000-of-00001 (deflated 4%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/1-00000-of-00001.gz (deflated 2%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/0-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/3-00000-of-00001.gz (deflated 2%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/MANIFEST (deflated 55%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/6-00000-of-00001.gz (deflated 2%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/4-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/9-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/updated_analyzer_cache/6/stroke-disease-pipeline-CsvExampleGen-examples-1-Split-train-STAR-44f0c8cb6e7a1a882841bf1b70ac6b62dc7bc80b8679f2523b1512213c23eeab/8-00000-of-00001.gz (deflated 3%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_anomalies/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_anomalies/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_anomalies/6/SchemaDiff.pb (deflated 56%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_stats/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_stats/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_stats/6/FeatureStats.pb (deflated 74%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_schema/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_schema/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_schema/6/schema.pbtxt (deflated 85%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_stats/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_stats/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/post_transform_stats/6/FeatureStats.pb (deflated 81%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transformed_metadata/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transformed_metadata/schema.pbtxt (deflated 85%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transformed_metadata/asset_map (deflated 86%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/metadata/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/metadata/schema.pbtxt (deflated 83%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/saved_model.pb (deflated 86%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/vocab_compute_and_apply_vocabulary_1_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/vocab_compute_and_apply_vocabulary_4_vocabulary (deflated 18%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/vocab_compute_and_apply_vocabulary_vocabulary (deflated 6%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/vocab_compute_and_apply_vocabulary_3_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/assets/vocab_compute_and_apply_vocabulary_2_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/variables/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.index (deflated 33%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transform_graph/6/transform_fn/variables/variables.data-00000-of-00001 (deflated 75%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_schema/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_schema/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/pre_transform_schema/6/schema.pbtxt (deflated 83%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/.system/executor_execution/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/6/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/6/Split-train/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/6/Split-train/transformed_examples-00000-of-00001.gz (deflated 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/6/Split-eval/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Transform/transformed_examples/6/Split-eval/transformed_examples-00000-of-00001.gz (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model_run/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model_run/8/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/logs/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/logs/train/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/logs/train/events.out.tfevents.1674293933.91c48f8f24a4.819.0.v2 (deflated 87%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/logs/validation/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/logs/validation/events.out.tfevents.1674293951.91c48f8f24a4.819.1.v2 (deflated 56%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/saved_model.pb (deflated 89%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/vocab_compute_and_apply_vocabulary_1_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/vocab_compute_and_apply_vocabulary_4_vocabulary (deflated 18%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/vocab_compute_and_apply_vocabulary_vocabulary (deflated 6%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/vocab_compute_and_apply_vocabulary_3_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/assets/vocab_compute_and_apply_vocabulary_2_vocabulary (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/keras_metadata.pb (deflated 91%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/variables/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/variables/variables.index (deflated 63%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/model/8/Format-Serving/variables/variables.data-00000-of-00001 (deflated 25%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Trainer/.system/executor_execution/8/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/3/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/3/Split-train/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/3/Split-train/FeatureStats.pb (deflated 74%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/3/Split-eval/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/statistics/3/Split-eval/FeatureStats.pb (deflated 74%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/StatisticsGen/.system/executor_execution/3/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/.system/executor_execution/9/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/eval_config.json (deflated 68%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/validations.tfrecord (deflated 8%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/plots-00000-of-00001.tfrecord (deflated 45%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/attributions-00000-of-00001.tfrecord (deflated 45%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/evaluation/9/metrics-00000-of-00001.tfrecord (deflated 73%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/blessing/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/blessing/9/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/Evaluator/blessing/9/BLESSED (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/.system/executor_execution/4/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/schema/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/schema/4/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/SchemaGen/schema/4/schema.pbtxt (deflated 83%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/5/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/5/Split-train/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/5/Split-train/SchemaDiff.pb (deflated 52%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/5/Split-eval/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/anomalies/5/Split-eval/SchemaDiff.pb (deflated 52%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/.system/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/.system/executor_execution/ (stored 0%)\n",
            "  adding: outputs/stroke-disease-pipeline/ExampleValidator/.system/executor_execution/5/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r data.zip data/\n",
        "!zip -r images.zip images/\n",
        "!zip -r outputs.zip outputs/\n",
        "!pip freeze > requirements.txt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
